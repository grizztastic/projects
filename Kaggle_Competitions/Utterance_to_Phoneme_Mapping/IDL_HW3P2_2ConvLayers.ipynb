{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cedmPnAz4EiT",
    "outputId": "3d869d47-029b-4953-9800-a31980afac4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4XRXhPWD4i4p"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJI-wToX4mA8"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YilOboVj4mDS",
    "outputId": "0f5edbc1-cf1e-44a8-a684-2988ec025e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m.\u001b[0m/   .bashrc  \u001b[01;34m.config\u001b[0m/  \u001b[01;34m.ipython\u001b[0m/  \u001b[01;34m.keras\u001b[0m/  \u001b[01;34m.node-gyp\u001b[0m/  .profile\n",
      "\u001b[01;34m..\u001b[0m/  \u001b[01;34m.cache\u001b[0m/  \u001b[01;34m.gsutil\u001b[0m/  \u001b[01;34m.jupyter\u001b[0m/  \u001b[01;34m.local\u001b[0m/  \u001b[01;34m.npm\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jY722HLu4mFr"
   },
   "outputs": [],
   "source": [
    "!mkdir .kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jETXP_fC4mIb",
    "outputId": "31df98ff-c4f0-4b2e-db22-34a108a0369c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m.\u001b[0m/   .bashrc  \u001b[01;34m.config\u001b[0m/  \u001b[01;34m.ipython\u001b[0m/  \u001b[01;34m.kaggle\u001b[0m/  \u001b[01;34m.local\u001b[0m/     \u001b[01;34m.npm\u001b[0m/\n",
      "\u001b[01;34m..\u001b[0m/  \u001b[01;34m.cache\u001b[0m/  \u001b[01;34m.gsutil\u001b[0m/  \u001b[01;34m.jupyter\u001b[0m/  \u001b[01;34m.keras\u001b[0m/   \u001b[01;34m.node-gyp\u001b[0m/  .profile\n"
     ]
    }
   ],
   "source": [
    "ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Gh9JVcD64mKl",
    "outputId": "16087d7a-ceb5-466e-ad5b-7ebea4411266"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a2cb72af-1518-4529-a42f-9fd7f043b04e\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-a2cb72af-1518-4529-a42f-9fd7f043b04e\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n"
     ]
    }
   ],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "d20zNLNg4mNA",
    "outputId": "59a42f0c-b6cb-478e-ba14-f31fb94921b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m.\u001b[0m/   .bashrc  \u001b[01;34m.config\u001b[0m/  \u001b[01;34m.ipython\u001b[0m/  \u001b[01;34m.kaggle\u001b[0m/     \u001b[01;34m.keras\u001b[0m/  \u001b[01;34m.node-gyp\u001b[0m/  .profile\n",
      "\u001b[01;34m..\u001b[0m/  \u001b[01;34m.cache\u001b[0m/  \u001b[01;34m.gsutil\u001b[0m/  \u001b[01;34m.jupyter\u001b[0m/  kaggle.json  \u001b[01;34m.local\u001b[0m/  \u001b[01;34m.npm\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJ5uS6eI4mO8"
   },
   "outputs": [],
   "source": [
    "mv kaggle.json .kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pd052muD4mRO",
    "outputId": "6aa448e4-bcc0-44f1-df67-27be29c47ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "of7y567Y4mTR",
    "outputId": "36fc0e3e-2ad2-4213-a1f9-b122dd8d992f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "f73szhQd4mVn",
    "outputId": "bb7a7633-6603-4846-e668-ad298e5b15ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "Downloading phoneme_list.py to /content\n",
      "  0% 0.00/1.40k [00:00<?, ?B/s]\n",
      "100% 1.40k/1.40k [00:00<00:00, 1.26MB/s]\n",
      "Downloading wsj0_train_merged_labels.npy.zip to /content\n",
      "  0% 0.00/2.39M [00:00<?, ?B/s]\n",
      "100% 2.39M/2.39M [00:00<00:00, 79.7MB/s]\n",
      "Downloading wsj0_dev.npy.zip to /content\n",
      " 77% 80.0M/104M [00:00<00:00, 171MB/s]\n",
      "100% 104M/104M [00:00<00:00, 262MB/s] \n",
      "Downloading wsj0_dev_merged_labels.npy to /content\n",
      "  0% 0.00/736k [00:00<?, ?B/s]\n",
      "100% 736k/736k [00:00<00:00, 230MB/s]\n",
      "Downloading wsj0_train.zip to /content\n",
      "100% 2.26G/2.26G [00:25<00:00, 54.2MB/s]\n",
      "100% 2.26G/2.26G [00:25<00:00, 95.1MB/s]\n",
      "Downloading wsj0_test.zip to /content\n",
      " 95% 47.0M/49.6M [00:00<00:00, 176MB/s]\n",
      "100% 49.6M/49.6M [00:00<00:00, 196MB/s]\n",
      "Downloading sample_submission.csv to /content\n",
      "  0% 0.00/4.14k [00:00<?, ?B/s]\n",
      "100% 4.14k/4.14k [00:00<00:00, 4.50MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c 11-785-s20-hw3p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ixuZVaOl4mXw",
    "outputId": "642e54ad-3184-4de9-f34a-a2a4a9dfb42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phoneme_list.py             wsj0_dev.npy.zip\n",
      "\u001b[0m\u001b[01;34msample_data\u001b[0m/                wsj0_test.zip\n",
      "sample_submission.csv       wsj0_train_merged_labels.npy.zip\n",
      "wsj0_dev_merged_labels.npy  wsj0_train.zip\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "7L7lgIwe4mZ6",
    "outputId": "86bc6562-77c4-4379-b3d7-e43257aa2285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wsj0_dev.npy.zip\n",
      "  inflating: wsj0_dev.npy            \n",
      "Archive:  wsj0_test.zip\n",
      "  inflating: wsj0_test               \n",
      "Archive:  wsj0_train.zip\n",
      "  inflating: wsj0_train              \n",
      "Archive:  wsj0_dev_merged_labels.npy\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of wsj0_dev_merged_labels.npy or\n",
      "        wsj0_dev_merged_labels.npy.zip, and cannot find wsj0_dev_merged_labels.npy.ZIP, period.\n",
      "Archive:  wsj0_train_merged_labels.npy.zip\n",
      "  inflating: wsj0_train_merged_labels.npy  \n"
     ]
    }
   ],
   "source": [
    "!unzip wsj0_dev.npy.zip\n",
    "!unzip wsj0_test.zip\n",
    "!unzip wsj0_train.zip\n",
    "!unzip wsj0_dev_merged_labels.npy\n",
    "!unzip wsj0_train_merged_labels.npy.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "P3Hupg5m4mcD",
    "outputId": "1f61b814-3f9d-4c98-b685-f2685a00a43f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phoneme_list.py             wsj0_dev.npy      wsj0_train\n",
      "\u001b[0m\u001b[01;34msample_data\u001b[0m/                wsj0_dev.npy.zip  wsj0_train_merged_labels.npy\n",
      "sample_submission.csv       wsj0_test         wsj0_train_merged_labels.npy.zip\n",
      "wsj0_dev_merged_labels.npy  wsj0_test.zip     wsj0_train.zip\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "awnK_yC16zLf",
    "outputId": "feda909a-380a-4448-ce27-47485ef7b5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ctcdecode'...\n",
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 1011 (delta 0), reused 3 (delta 0), pack-reused 1006\u001b[K\n",
      "Receiving objects: 100% (1011/1011), 730.55 KiB | 2.23 MiB/s, done.\n",
      "Resolving deltas: 100% (500/500), done.\n",
      "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
      "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
      "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
      "remote: Enumerating objects: 82, done.        \n",
      "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
      "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
      "remote: Enumerating objects: 72, done.        \n",
      "remote: Counting objects: 100% (72/72), done.        \n",
      "remote: Compressing objects: 100% (59/59), done.        \n",
      "remote: Total 13396 (delta 32), reused 37 (delta 13), pack-reused 13324        \n",
      "Receiving objects: 100% (13396/13396), 5.42 MiB | 11.30 MiB/s, done.\n",
      "Resolving deltas: 100% (7677/7677), done.\n",
      "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
      "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=ce14e130d231fa84bfa78d4b65da44d2f71a52752ece1ee2988d3f29b108bc9f\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "/content/ctcdecode\n",
      "Processing /content/ctcdecode\n",
      "Building wheels for collected packages: ctcdecode\n",
      "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ctcdecode: filename=ctcdecode-0.4-cp36-cp36m-linux_x86_64.whl size=12157529 sha256=80601cc47e68fa96b1eea31e259d5c8ae6c3d78f70a030380a8d92c286cb456b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4vm6g76o/wheels/c3/6c/94/7d57d4f20a87a22ef1722eaad22052b4c435892b55400e5f4e\n",
      "Successfully built ctcdecode\n",
      "Installing collected packages: ctcdecode\n",
      "Successfully installed ctcdecode-0.4\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget --user\n",
    "%cd ctcdecode\n",
    "!pip install . --user\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "iG4rMgPP7aes",
    "outputId": "796b8f7a-58e1-4b33-dc79-b47e010a74ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM2_checkpoint.pth    wsj0_dev.npy\n",
      "ConvLSTM_checkpoint.pth     wsj0_dev.npy.zip\n",
      "\u001b[0m\u001b[01;34mctcdecode\u001b[0m/                  wsj0_test\n",
      "hw3dataloader.py            wsj0_test.zip\n",
      "hw3model.py                 wsj0_train\n",
      "phoneme_list.py             wsj0_train_merged_labels.npy\n",
      "\u001b[01;34msample_data\u001b[0m/                wsj0_train_merged_labels.npy.zip\n",
      "sample_submission.csv       wsj0_train.zip\n",
      "wsj0_dev_merged_labels.npy\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "oGOoPlZZCHIG",
    "outputId": "ad3b911f-e4b0-41ce-b4c3-e9617928fd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
      "\r",
      "\u001b[K     |██████▊                         | 10kB 30.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 20kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 30kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 40kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (46.1.3)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144803 sha256=9cb012337d64c521481accec737152f89cf5fe32b29f6d1e6dc4cf4dc04e425a\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxBSfYAD4meK"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "import Levenshtein as Lev\n",
    "from torch.autograd import Variable\n",
    "# from hw3model import *\n",
    "from hw3dataloader import *\n",
    "from phoneme_list import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rc68yLJR4mgL"
   },
   "outputs": [],
   "source": [
    "best_accuracy = 0  # threshold to determine if a model checkpoint is formed\n",
    "start_epoch = 0  # last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5OWGaMV6x2Z"
   },
   "outputs": [],
   "source": [
    "'''Load the data'''\n",
    "#Load the datasets\n",
    "features_dev = np.load('wsj0_dev.npy', allow_pickle = True)\n",
    "labels_dev = np.load('wsj0_dev_merged_labels.npy', allow_pickle = True)\n",
    "features_train = np.load('wsj0_train', allow_pickle = True)\n",
    "labels_train = np.load('wsj0_train_merged_labels.npy', allow_pickle = True)\n",
    "features_test = np.load('wsj0_test', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqyYNY-XdZJs"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, embedding_size, channel_size, hidden_size, stride=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(embedding_size, channel_size, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(channel_size)\n",
    "        self.conv2 = nn.Conv1d(channel_size, hidden_size, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.tanh = nn.Hardtanh(inplace=True)\n",
    "        self.lstm= nn.LSTM(hidden_size, hidden_size, bidirectional=True, num_layers=4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size , hidden_size//2)\n",
    "        self.linear3 = nn.Linear(hidden_size//2, 47)\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        X = X.permute(1, 2, 0)\n",
    "        X = X.contiguous()\n",
    "        out = self.tanh(self.bn1(self.conv1(X)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = out.permute(0, 2, 1)\n",
    "        packed_out = self.lstm(pack_padded_sequence(out, lengths, enforce_sorted=False))[0]\n",
    "        out, out_lens = pad_packed_sequence(packed_out)\n",
    "        out = self.linear1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear3(out).log_softmax(2)\n",
    "        return out, out_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LG1AZA5x6x4z"
   },
   "outputs": [],
   "source": [
    "'''This function trains my model and outputs a csv of test predictions along with saving the model state dict if the new accuracy\n",
    "   is better than the previous epochs accuracy. Code was very similar to my hw1 code except I added the torch loader'''\n",
    "\n",
    "def train(model, training_data_loader, validation_data_loader, test_data_loader, criterion, optimizer, num_epochs, device, log_interval, scheduler):\n",
    "    model.train()  # initialize training of model\n",
    "    global best_accuracy\n",
    "    global start_epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (data, target, X_train_lens, Y_train_lens) in enumerate(training_data_loader):  # loop over the training data\n",
    "            optimizer.zero_grad()  # clear gradients\n",
    "            data = data.to(device)  # make data run on device (GPU or CPU)\n",
    "            target = target.to(device)  # make target data(labels) run on device\n",
    "            X_train_lens = X_train_lens.to(device)\n",
    "            target_lengths = Y_train_lens.to(device)\n",
    "            outputs, output_lengths = model(data, X_train_lens)  # forward pass through model to get predictions from our data(features)\n",
    "            loss = criterion(outputs, target, output_lengths, target_lengths)  # get loss from forward pass and the target values\n",
    "            running_loss += loss.item()  # sum the loss\n",
    "            loss.backward()  # backward pass through to calculate weights and bias updates needed\n",
    "            optimizer.step()  # step and update weights to linear and BN layers\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Start Epoch: {} Train Epoch: {} Batch Number: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    start_epoch, epoch + 1, batch_idx, batch_idx * len(data), len(training_data_loader.dataset),\n",
    "                                 100. * batch_idx / len(training_data_loader), loss.item()))\n",
    "            del data\n",
    "            del target\n",
    "            del X_train_lens\n",
    "            del target_lengths\n",
    "\n",
    "        end_time = time.time()\n",
    "        running_loss /= len(training_data_loader)  # calculate running loss\n",
    "        print('Training Loss: ', running_loss, 'Time: ', end_time - start_time, 's')\n",
    "        scheduler.step()\n",
    "        #val_loss, val_acc = test(model, validation_data_loader, criterion, device)\n",
    "        test(model, validation_data_loader, criterion, device)\n",
    "        # print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(val_loss, val_acc))\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('ConvLSTM3_model_checkpoint'):\n",
    "            os.mkdir('ConvLSTM3_model_checkpoint')\n",
    "        torch.save(state, './ConvLSTM3_model_checkpoint/ConvLSTM3_checkpoint.pth')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4C0UJy66yF6"
   },
   "outputs": [],
   "source": [
    "def test(model, dev_data_loader, criterion, device):\n",
    "    global best_accuracy\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        character_error = 0.0\n",
    "        decoder = CTCBeamDecoder(PHONEME_MAP + [' '], beam_width=40,blank_id=0, log_probs_input=True)\n",
    "        for batch_idx, (data, target, X_dev_lens, Y_dev_lens) in enumerate(dev_data_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            X_dev_lens = X_dev_lens.to(device)\n",
    "            target_lengths = Y_dev_lens.to(device)\n",
    "            outputs, output_lengths = model(data, X_dev_lens)\n",
    "            total_predictions += target.size(0)  # calculate number of predictions made\n",
    "            loss = criterion(outputs, target, output_lengths, target_lengths).detach()  # calculate loss from torch.nn.CrossEntropy\n",
    "            running_loss += loss.item()\n",
    "            test_Y, _, _, test_Y_lens = decoder.decode(outputs.transpose(0, 1), output_lengths)\n",
    "            phoneme_out = \"\"\n",
    "            ls = 0.\n",
    "            char_err = []\n",
    "            for i in range(test_Y.size(0)):\n",
    "                pred = \" \".join(PHONEME_MAP[o] for o in test_Y[i, 0, :test_Y_lens[i, 0]])\n",
    "                pred = pred.replace(' ', '')\n",
    "                #print('Pred: ', pred)\n",
    "                true = \"\".join(PHONEME_MAP[l] for l in target[i])\n",
    "                #print('True: ', true.replace('_',''))\n",
    "                ls = Lev.distance(pred.replace(' ', ''), true.replace('_',''))  \n",
    "                char_err.append(ls)\n",
    "            del data\n",
    "            del target\n",
    "            del X_dev_lens\n",
    "            del target_lengths\n",
    "        running_loss /= len(dev_data_loader)\n",
    "        character_error = np.sum(np.asarray(char_err))\n",
    "        char_err_val = (character_error / total_predictions) * 100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Character Error: ', char_err_val, '%')\n",
    "        model.train()\n",
    "        return running_loss, char_err_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxhA7DK07mIy"
   },
   "outputs": [],
   "source": [
    "def test_predictions(model, test_data_loader, criterion, device):\n",
    "    predictions = []\n",
    "    predictions_final = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        character_error = 0.0\n",
    "        decoder = CTCBeamDecoder(PHONEME_MAP + [' '], beam_width=40,blank_id=0, log_probs_input=True)\n",
    "        for batch_idx, (data, X_test_lens) in enumerate(test_data_loader):\n",
    "            data = data.to(device)\n",
    "            X_test_lens = X_test_lens.to(device)\n",
    "            outputs, output_lengths = model(data, X_test_lens)\n",
    "            test, _, _, test_lengths = decoder.decode(outputs.transpose(0, 1), output_lengths)\n",
    "            phoneme_out = \"\"\n",
    "            prediction = []\n",
    "            ls = 0.\n",
    "            char_err = []\n",
    "            for i in range(test.size(0)):\n",
    "                pred = \"\".join(PHONEME_MAP[o] for o in test[i, 0, :test_lengths[i, 0]])\n",
    "                pred = pred.replace(' ', '')\n",
    "                predictions.append(pred + '_') \n",
    "            del data\n",
    "            del X_test_lens\n",
    "        predictions_final = np.asarray(predictions).T\n",
    "        columns = np.asarray(np.asmatrix(range(len(predictions_final))).T)  #column indices\n",
    "        data_stuff = np.column_stack((columns, predictions_final))\n",
    "        np.savetxt('Kaggle_Submission4HW3.csv', data_stuff, delimiter=\",\", fmt='%s', header=\"Id,Predicted\", comments='') #save a csv file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZW0y4O7Z6yWJ"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_accuracy\n",
    "    global start_epoch\n",
    "\n",
    "    lr = 0.0002\n",
    "    train_batch_size = 64\n",
    "    no_cuda = False\n",
    "    seed = 11785\n",
    "    log_interval = 50\n",
    "    resume = True\n",
    "\n",
    "    cuda = not no_cuda and torch.cuda.is_available()  # check if GPU is available\n",
    "    print(cuda)\n",
    "    torch.manual_seed(seed)  # set seed manually\n",
    "    device = torch.device(\"cuda:0\" if cuda else \"cpu\")  # gives whether the device is CPU or GPU\n",
    "\n",
    "    if cuda:\n",
    "        kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "    else:\n",
    "        kwargs = {}\n",
    "\n",
    "    '''Data loaders below call in the data set, and give other arguments like batch size, shuffle, num_workers, etc.'''\n",
    "    training_data_loader = torch.utils.data.DataLoader(TrainData(features_train, labels_train), batch_size=train_batch_size, shuffle=True, collate_fn=collate_train, **kwargs)\n",
    "    dev_data_loader = torch.utils.data.DataLoader(TrainData(features_dev, labels_dev), batch_size=train_batch_size, shuffle=True, collate_fn=collate_train, **kwargs)\n",
    "    test_data_loader = torch.utils.data.DataLoader(TestData(features_test), batch_size=train_batch_size, shuffle=False, collate_fn=collate_test, **kwargs)\n",
    "\n",
    "    embedding_size = 40\n",
    "    #input_channel_size = 64\n",
    "    channel_size = 128\n",
    "    hidden_size = 256\n",
    "    # model = MyModel(embedding_size, input_channel_size, channel_size, hidden_size)\n",
    "    model = MyModel(embedding_size, channel_size, hidden_size, stride=1)\n",
    "    model.apply(init_weights)\n",
    "    criterion = torch.nn.CTCLoss()\n",
    "    Optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(Optimizer, step_size=5, gamma=0.8)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "\n",
    "    if resume:\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        assert os.path.isdir('ConvLSTM3_model_checkpoint'), 'Error: no checkpoint directory found!'\n",
    "        checkpoint = torch.load('./ConvLSTM3_model_checkpoint/ConvLSTM3_checkpoint.pth')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "    n_epochs = 5  # set num epochs\n",
    "    #train(model, training_data_loader, dev_data_loader, test_data_loader, criterion, Optimizer, n_epochs, device, log_interval, scheduler)  # train model\n",
    "    #test(model, dev_data_loader, criterion, device)\n",
    "    test_predictions(model, test_data_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvhXpS1VOrfH"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "t9YZtspw4mil",
    "outputId": "4d470075-c30b-485c-e309-e365f148b9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "MyModel(\n",
      "  (conv1): Conv1d(40, 128, kernel_size=(1,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh): Hardtanh(min_val=-1.0, max_val=1.0, inplace=True)\n",
      "  (lstm): LSTM(256, 256, num_layers=4, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (linear3): Linear(in_features=128, out_features=47, bias=True)\n",
      ")\n",
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Fc7j9fA5yj5Z",
    "outputId": "62e5c023-8c6e-44e4-d40a-829d0b3a9007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '+', '~', '!', '-', '@', 'a', 'A', 'h', 'o', 'w', 'y', 'b', 'c', 'd', 'D', 'e', 'r', 'E', 'f', 'g', 'H', 'i', 'I', 'j', 'k', 'l', 'm', 'n', 'G', 'O', 'Y', 'p', 'R', 's', 'S', '.', 't', 'T', 'u', 'U', 'v', 'W', '?', 'z', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(PHONEME_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkcsLXTiUL22"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "IDL_HW3P2_2ConvLayers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
