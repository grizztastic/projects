{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "SLIRGbC3D34Q",
    "outputId": "cabba7d0-0f6a-41e9-fa97-3cd8a9a7ad2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  hw5_part_two_release.zip\n",
      "  inflating: data/questions-words.txt  \n",
      "  inflating: data/definitional_pairs.json  \n",
      "  inflating: data/equalize_pairs.json  \n",
      "  inflating: data/gender_specific_full.json  \n",
      "  inflating: data/profession_words.json  \n",
      "  inflating: hw5_part2.ipynb         \n",
      "  inflating: hw5_part1.py            \n",
      "  inflating: hw5_part1_utils.py      \n",
      "  inflating: hw5_part2.py            \n",
      "  inflating: hw5_part3.py            \n",
      "  inflating: adult.npz               \n",
      "  inflating: homework.pdf            \n"
     ]
    }
   ],
   "source": [
    "!unzip hw5_part_two_release.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "epvizLkvEwVe",
    "outputId": "d66046e3-fd24-443c-af67-563b69963da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQYCuTB8FTGW"
   },
   "outputs": [],
   "source": [
    "import hw5_part1_utils\n",
    "\n",
    "from typing import Tuple\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zgKh_DviE1ab",
    "outputId": "889f1ff9-9171-4c94-8f32-2970133c4798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target model...\n",
      "WARNING:tensorflow:From /content/hw5_part1_utils.py:95: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Building attack model...\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 86us/sample - loss: 0.5538 - acc: 0.7422\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 82us/sample - loss: 0.5321 - acc: 0.7566\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 84us/sample - loss: 0.5378 - acc: 0.7540\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 83us/sample - loss: 0.5322 - acc: 0.7551\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 84us/sample - loss: 0.5441 - acc: 0.7505\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 84us/sample - loss: 0.5345 - acc: 0.7514\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 85us/sample - loss: 0.5391 - acc: 0.7483\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 85us/sample - loss: 0.5377 - acc: 0.7509\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 7s 87us/sample - loss: 0.5376 - acc: 0.7515\n",
      "Train on 80000 samples\n",
      "80000/80000 [==============================] - 8s 95us/sample - loss: 0.5394 - acc: 0.7509\n",
      "Evaluating target model...\n",
      "  Train Accuracy: 0.9529\n",
      "  Test Accuracy:  0.4144\n",
      "Index: 0\n",
      "Index: 100\n",
      "Index: 200\n",
      "Index: 300\n",
      "Index: 400\n",
      "Index: 500\n",
      "Index: 600\n",
      "Index: 700\n",
      "Index: 800\n",
      "Index: 900\n",
      "Index: 1000\n",
      "Index: 1100\n",
      "Index: 1200\n",
      "Index: 1300\n",
      "Index: 1400\n",
      "Index: 1500\n",
      "Index: 1600\n",
      "Index: 1700\n",
      "Index: 1800\n",
      "Index: 1900\n",
      "Index: 2000\n",
      "Index: 2100\n",
      "Index: 2200\n",
      "Index: 2300\n",
      "Index: 2400\n",
      "Index: 2500\n",
      "Index: 2600\n",
      "Index: 2700\n",
      "Index: 2800\n",
      "Index: 2900\n",
      "Index: 3000\n",
      "Index: 3100\n",
      "Index: 3200\n",
      "Index: 3300\n",
      "Index: 3400\n",
      "Index: 3500\n",
      "Index: 3600\n",
      "Index: 3700\n",
      "Index: 3800\n",
      "Index: 3900\n",
      "Index: 4000\n",
      "Index: 4100\n",
      "Index: 4200\n",
      "Index: 4300\n",
      "Index: 4400\n",
      "Index: 4500\n",
      "Index: 4600\n",
      "Index: 4700\n",
      "Index: 4800\n",
      "Index: 4900\n",
      "Index: 5000\n",
      "Index: 5100\n",
      "Index: 5200\n",
      "Index: 5300\n",
      "Index: 5400\n",
      "Index: 5500\n",
      "Index: 5600\n",
      "Index: 5700\n",
      "Index: 5800\n",
      "Index: 5900\n",
      "Index: 6000\n",
      "Index: 6100\n",
      "Index: 6200\n",
      "Index: 6300\n",
      "Index: 6400\n",
      "Index: 6500\n",
      "Index: 6600\n",
      "Index: 6700\n",
      "Index: 6800\n",
      "Index: 6900\n",
      "Index: 7000\n",
      "Index: 7100\n",
      "Index: 7200\n",
      "Index: 7300\n",
      "Index: 7400\n",
      "Index: 7500\n",
      "Index: 7600\n",
      "Index: 7700\n",
      "Index: 7800\n",
      "Index: 7900\n",
      "Index: 8000\n",
      "Index: 8100\n",
      "Index: 8200\n",
      "Index: 8300\n",
      "Index: 8400\n",
      "Index: 8500\n",
      "Index: 8600\n",
      "Index: 8700\n",
      "Index: 8800\n",
      "Index: 8900\n",
      "Index: 9000\n",
      "Index: 9100\n",
      "Index: 9200\n",
      "Index: 9300\n",
      "Index: 9400\n",
      "Index: 9500\n",
      "Index: 9600\n",
      "Index: 9700\n",
      "Index: 9800\n",
      "Index: 9900\n",
      "Index: 0\n",
      "Index: 100\n",
      "Index: 200\n",
      "Index: 300\n",
      "Index: 400\n",
      "Index: 500\n",
      "Index: 600\n",
      "Index: 700\n",
      "Index: 800\n",
      "Index: 900\n",
      "Index: 1000\n",
      "Index: 1100\n",
      "Index: 1200\n",
      "Index: 1300\n",
      "Index: 1400\n",
      "Index: 1500\n",
      "Index: 1600\n",
      "Index: 1700\n",
      "Index: 1800\n",
      "Index: 1900\n",
      "Index: 2000\n",
      "Index: 2100\n",
      "Index: 2200\n",
      "Index: 2300\n",
      "Index: 2400\n",
      "Index: 2500\n",
      "Index: 2600\n",
      "Index: 2700\n",
      "Index: 2800\n",
      "Index: 2900\n",
      "Index: 3000\n",
      "Index: 3100\n",
      "Index: 3200\n",
      "Index: 3300\n",
      "Index: 3400\n",
      "Index: 3500\n",
      "Index: 3600\n",
      "Index: 3700\n",
      "Index: 3800\n",
      "Index: 3900\n",
      "Index: 4000\n",
      "Index: 4100\n",
      "Index: 4200\n",
      "Index: 4300\n",
      "Index: 4400\n",
      "Index: 4500\n",
      "Index: 4600\n",
      "Index: 4700\n",
      "Index: 4800\n",
      "Index: 4900\n",
      "Index: 5000\n",
      "Index: 5100\n",
      "Index: 5200\n",
      "Index: 5300\n",
      "Index: 5400\n",
      "Index: 5500\n",
      "Index: 5600\n",
      "Index: 5700\n",
      "Index: 5800\n",
      "Index: 5900\n",
      "Index: 6000\n",
      "Index: 6100\n",
      "Index: 6200\n",
      "Index: 6300\n",
      "Index: 6400\n",
      "Index: 6500\n",
      "Index: 6600\n",
      "Index: 6700\n",
      "Index: 6800\n",
      "Index: 6900\n",
      "Index: 7000\n",
      "Index: 7100\n",
      "Index: 7200\n",
      "Index: 7300\n",
      "Index: 7400\n",
      "Index: 7500\n",
      "Index: 7600\n",
      "Index: 7700\n",
      "Index: 7800\n",
      "Index: 7900\n",
      "Index: 8000\n",
      "Index: 8100\n",
      "Index: 8200\n",
      "Index: 8300\n",
      "Index: 8400\n",
      "Index: 8500\n",
      "Index: 8600\n",
      "Index: 8700\n",
      "Index: 8800\n",
      "Index: 8900\n",
      "Index: 9000\n",
      "Index: 9100\n",
      "Index: 9200\n",
      "Index: 9300\n",
      "Index: 9400\n",
      "Index: 9500\n",
      "Index: 9600\n",
      "Index: 9700\n",
      "Index: 9800\n",
      "Index: 9900\n",
      "\n",
      "True positive rate: 0.9109, true negative rate: 0.6506\n",
      "Shadow Attack Accuracy: 0.7808, precision: 0.7228 (baseline: 0.7692, 0.6969)\n"
     ]
    }
   ],
   "source": [
    "# YOUR IMPLEMENTATION FOR THE SHADOW MODEL ATTACK GOES HERE ###################\n",
    "\n",
    "\n",
    "def synthesize_attack_data(\n",
    "    target_model: hw5_part1_utils.TargetModel,\n",
    "    shadow_data: np.ndarray,\n",
    "    shadow_labels: np.ndarray,\n",
    "    num_shadow_models: int = 4\n",
    "):\n",
    "    \"\"\"Synthesize attack data.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        target_model {TargetModel} -- an instance of the TargetModel class;\n",
    "          behaves as a keras model but additionally has a train_shadow_model\n",
    "          function, which takes a subset of the shadow data and labels and\n",
    "          returns a model with identical architecture and hyperparameters to\n",
    "          the original target model, but that is trained on the given shadow\n",
    "          data.\n",
    "\n",
    "        shadow_data {np.ndarray} -- data available to the attack to train\n",
    "          shadow models. If the arget model's training set is size N x D,\n",
    "          shadow_data is 2N x D.\n",
    "\n",
    "        shadow_labels {np.ndarray} -- the corresponding labels to the\n",
    "          shadow_data, given as a numpy array of 2N integers in the range 0 to\n",
    "          C where C is the number of classes.\n",
    "\n",
    "        num_shadow_models {int} -- the number of shadow models to use when\n",
    "          constructing the attack model's dataset.\n",
    "\n",
    "    Returns: three np.ndarrays; let M = 2N * num_shadow_models\n",
    "\n",
    "        attack_data {np.ndarray} [M, 2C] -- shadow data label probability and\n",
    "           label one-hot\n",
    "\n",
    "        attack_classes {np.ndarray} [M, 1 of {0,1,...,C-1}] -- shadow data\n",
    "           labels\n",
    "\n",
    "        attack_labels {np.ndarray} [M, 1 of {0,1}] -- attack data labels\n",
    "           (training membership)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    C = shadow_labels.max() + 1\n",
    "\n",
    "    attack_data: np.ndarray = None\n",
    "    attack_classes: np.ndarray = None\n",
    "    attack_labels: np.ndarray = None\n",
    "\n",
    "    # SOLUTION\n",
    "    A = []\n",
    "    S_classes = []\n",
    "    S_labels = []\n",
    "    for i in range(num_shadow_models):\n",
    "        data_split = hw5_part1_utils.DataSplit(shadow_labels, np.random.seed(i)) #get in and out indexes \n",
    "        S_in_idx = data_split.in_idx \n",
    "        S_out_idx = data_split.out_idx\n",
    "\n",
    "        S_in = shadow_data[S_in_idx] #get the S in data using the S in indexes for the shadow data\n",
    "        S_in_labels = shadow_labels[S_in_idx] #get the S in labels using the S in indexes for the shadow labels\n",
    "        S_in_labels_one_hot = to_categorical(S_in_labels, 10) #get one hot encondings of S in labels\n",
    " \n",
    "        S_out = shadow_data[S_out_idx] #get the S out data using the S out indexes for the shadow data\n",
    "        S_out_labels = shadow_labels[S_out_idx] #get the S out labels using the S out indexes for the shadow labels\n",
    "        S_out_labels_one_hot = to_categorical(shadow_labels[S_out_idx], 10) #get one hot encondings of S out labels\n",
    "        \n",
    "        trained_model = target_model.train_shadow_model(S_in, S_in_labels) #train the shadow model with generated S in and S in labels data\n",
    "        S_in_preds = trained_model.predict(S_in) #predict the output from trained model using S in data\n",
    "        S_out_preds = trained_model.predict(S_out) #predict the output from trained model using S out data\n",
    "        A_in = np.hstack((S_in_preds, S_in_labels_one_hot)) #get A in data by stacking the preds and labels\n",
    "        A_out = np.hstack((S_out_preds, S_out_labels_one_hot)) #get A out data by stacking the preds and labels\n",
    "        A.append(np.vstack((A_in, A_out)))\n",
    "        S_classes.append(np.hstack((S_in_labels, S_out_labels))) #append stacked S in and out labels for attack_classes\n",
    "        S_labels.append(np.hstack((np.ones(len(S_in_labels)), np.zeros(len(S_out_labels))))) #one hot vector for attack classes\n",
    "    attack_data = np.vstack((A))\n",
    "    attack_classes = np.hstack((S_classes))\n",
    "    attack_labels = np.hstack((S_labels))\n",
    "    # END OF SOLUTION\n",
    "\n",
    "    return attack_data, attack_classes, attack_labels\n",
    "\n",
    "\n",
    "def build_attack_models(\n",
    "    target_model: hw5_part1_utils.TargetModel,\n",
    "    shadow_data: np.ndarray,\n",
    "    shadow_labels: np.ndarray,\n",
    "    num_shadow_models: int = 4\n",
    "):\n",
    "    \"\"\"Build attacker models.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        target_model {TargetModel} -- an instance of the TargetModel class;\n",
    "          behaves as a keras model but additionally has a train_shadow_model\n",
    "          function, which takes a subset of the shadow data and labels and\n",
    "          returns a model with identical architecture and hyperparameters to\n",
    "          the original target model, but that is trained on the given shadow\n",
    "          data.\n",
    "\n",
    "        shadow_data {np.ndarray} -- data available to the attack to train\n",
    "          shadow models. If the arget model's training set is size N x D,\n",
    "          shadow_data is 2N x D.\n",
    "\n",
    "        shadow_labels {np.ndarray} -- the corresponding labels to the\n",
    "          shadow_data, given as a numpy array of 2N integers in the range 0 to\n",
    "          C where C is the number of classes.\n",
    "\n",
    "        num_shadow_models {int} -- the number of shadow models to use when\n",
    "          constructing the attack model's dataset.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        {tuple} -- a tuple of C keras models, where the c^th model predicts the\n",
    "        probability that an instance of class c was a training set member.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    attack_data, attack_classes, attack_labels = \\\n",
    "        synthesize_attack_data(\n",
    "            target_model,\n",
    "            shadow_data,\n",
    "            shadow_labels,\n",
    "            num_shadow_models=4\n",
    "        )\n",
    "\n",
    "    # to return\n",
    "    attack_models: Tuple[Model] = None\n",
    "\n",
    "    C = shadow_labels.max() + 1\n",
    "\n",
    "    # SOLUTION\n",
    "    attack_models = []\n",
    "    for i in range(C): #range over class numbers\n",
    "        attack_model = get_attack_architecture(C) #initialize attack model for class\n",
    "        attack_model.fit(attack_data, attack_labels) #fit model with attack data and labels\n",
    "        attack_models.append(attack_model) #append attack model\n",
    "    attack_models = tuple(attack_models) #turn into a tuple\n",
    "    # END OF SOLUTION\n",
    "\n",
    "    return attack_models\n",
    "\n",
    "'''Initial model architecture taken from hw5_part1_utils.py and modified to meet the \n",
    "   assigment requests'''\n",
    "def get_attack_architecture(C):\n",
    "    l_in = Input((2*C,)) #input layer size based on class number\n",
    "    l_inter = Dense(4*C, activation='relu')(l_in) #relu activation with hidden layer dependendent on class\n",
    "    l_out = Dense(1, activation='sigmoid')(l_inter) #output layer of size 1\n",
    "\n",
    "    m = Model(l_in, l_out) #initialize model\n",
    "\n",
    "    m.compile(\n",
    "        loss='binary_crossentropy', #use binary cross entropy\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'],\n",
    "        experimental_run_tf_function=False \n",
    "    )\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def evaluate_membership(attack_models, y_pred, y):\n",
    "    \"\"\"Evaluate the attacker about the membership inference\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        attack_model {tuple} -- a tuple of C keras models, where C is the\n",
    "          number of classes.\n",
    "\n",
    "        y_pred {np.ndarray} -- an N x C numpy array with the predictions of the\n",
    "          model on the N instances we are performing the inference attack on.\n",
    "\n",
    "        y {np.ndarray} -- the true labels for each of the instances given as a\n",
    "          numpy array of N integers.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        {np.ndarray} -- an array of N floats in the range [0,1] representing\n",
    "          the estimated probability that each of the N given instances is a\n",
    "          training set member.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # To return\n",
    "    preds: np.ndarray = None\n",
    "\n",
    "    # SOLUTION\n",
    "    predictions = []\n",
    "    attack_models = list(attack_models)\n",
    "    #used to give proper y value size to model for predictions\n",
    "    y_for_model = [np.hstack((yp, to_categorical(Y, 10))) for yp, Y in zip(y_pred, y)] \n",
    "    for i in range(len(y)):\n",
    "        if i%100 == 0:\n",
    "            print(\"Index: {}\".format(i))\n",
    "        predictions.append(attack_models[y[i]].predict(np.asmatrix(y_for_model[i])))\n",
    "    preds = np.array(predictions)\n",
    "\n",
    "    # END OF SOLUTION\n",
    "\n",
    "    return preds\n",
    "\n",
    "# YOU DO NOT NEED TO MODIFY THE REST OF THIS CODE. ############################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the dataset.\n",
    "    data = hw5_part1_utils.CIFARData()\n",
    "\n",
    "    # Make a target model for the dataset.\n",
    "    target_model = \\\n",
    "        hw5_part1_utils.CIFARModel(\n",
    "            epochs=48,\n",
    "            batch_size=2048,\n",
    "            noload=True, # prevents loading an existing pre-trained target\n",
    "                         # model\n",
    "        ).init(\n",
    "            data.train, data.labels_train,\n",
    "            # data.test, data.labels_test # validation data\n",
    "        )\n",
    "\n",
    "    tqdm.write('Building attack model...')\n",
    "    attack_models = build_attack_models(\n",
    "        target_model,\n",
    "        data.shadow,\n",
    "        data.labels_shadow\n",
    "    )\n",
    "\n",
    "    tqdm.write('Evaluating target model...')\n",
    "    y_pred_in = target_model.predict(data.train)\n",
    "    y_pred_out = target_model.predict(data.test)\n",
    "\n",
    "    tqdm.write('  Train Accuracy: {:.4f}'.format(\n",
    "        (y_pred_in.argmax(axis=1) == data.labels_train).mean()))\n",
    "    tqdm.write('  Test Accuracy:  {:.4f}'.format(\n",
    "        (y_pred_out.argmax(axis=1) == data.labels_test).mean()))\n",
    "\n",
    "    in_preds = evaluate_membership(\n",
    "        attack_models,\n",
    "        y_pred_in,\n",
    "        data.labels_train\n",
    "    )\n",
    "    out_preds = evaluate_membership(\n",
    "        attack_models,\n",
    "        y_pred_out,\n",
    "        data.labels_test\n",
    "    )\n",
    "\n",
    "    wrongs_in = y_pred_in.argmax(axis=1) != data.labels_train\n",
    "    wrongs_out = y_pred_out.argmax(axis=1) != data.labels_test\n",
    "\n",
    "    true_positives = (in_preds > 0.5).mean()\n",
    "    true_negatives = (out_preds < 0.5).mean()\n",
    "    attack_acc = (true_positives + true_negatives) / 2.\n",
    "\n",
    "    attack_precision = (in_preds > 0.5).sum() / (\n",
    "        (in_preds > 0.5).sum() + (out_preds > 0.5).sum()\n",
    "    )\n",
    "\n",
    "    # Compare to a baseline that merely guesses correct classified instances\n",
    "    # are in and incorrectly classified instances are out.\n",
    "    baseline_true_positives = \\\n",
    "        (y_pred_in.argmax(axis=1) == data.labels_train).mean()\n",
    "    baseline_true_negatives = \\\n",
    "        (y_pred_out.argmax(axis=1) != data.labels_test).mean()\n",
    "    baseline_attack_acc = \\\n",
    "        (baseline_true_positives + baseline_true_negatives) / 2.\n",
    "\n",
    "    baseline_precision = \\\n",
    "        (y_pred_in.argmax(axis=1) == data.labels_train).sum() / (\n",
    "            (y_pred_in.argmax(axis=1) == data.labels_train).sum() +\n",
    "            (y_pred_out.argmax(axis=1) == data.labels_test).sum()\n",
    "        )\n",
    "\n",
    "    tqdm.write(\n",
    "      f\"\\nTrue positive rate: {true_positives:0.4f}, \" +\n",
    "      f\"true negative rate: {true_negatives:0.4f}\"\n",
    "    )\n",
    "    tqdm.write(\n",
    "      f\"Shadow Attack Accuracy: {attack_acc:0.4f}, precision: {attack_precision:0.4f} \" +\n",
    "      f\"(baseline: {baseline_attack_acc:0.4f}, {baseline_precision:0.4f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "6onOSy3xE1l-",
    "outputId": "1460bc89-d9a7-43c2-c0c0-a4941be7f798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "(1, 20)\n",
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "C = 10\n",
    "y_pred = np.random.rand(N, C)\n",
    "y = np.random.rand(N)\n",
    "y_for_model = [np.hstack((yp, to_categorical(Y, 10))) for yp, Y in zip(y_pred, y)]\n",
    "# for yp, Y in zip(y_pred, y):\n",
    "    #y_for_model = np.hstack((yp, to_categorical(Y, 10)))\n",
    "for i in range(len(y)):\n",
    "    print(np.asmatrix(y_for_model[i]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-1cxlk987ru"
   },
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "9YNRjD2pE1o2",
    "outputId": "bc19b668-22f8-4c07-b69d-e667d50df6a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for json\u001b[0m\n",
      "--2020-05-10 14:51:59--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.88.117\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.88.117|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  45.1MB/s    in 36s     \n",
      "\n",
      "2020-05-10 14:52:35 (43.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim json\n",
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
    "!mv GoogleNews-vectors-negative300.bin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnXPjtd0E1up"
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QRNhoZRZAPtG",
    "outputId": "4087d0c3-a321-44c5-b291-6ff169e12b77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors loaded\n",
      "==================================================\n",
      "Original Embeddings\n",
      "Appropriate Analogies\n",
      "'woman' is to 'her' as 'man' is to 'his'\n",
      "'woman' is to 'herself' as 'man' is to 'himself'\n",
      "'woman' is to 'spokeswoman' as 'man' is to 'spokesman'\n",
      "'woman' is to 'daughter' as 'man' is to 'son'\n",
      "'woman' is to 'mother' as 'man' is to 'father'\n",
      "'woman' is to 'niece' as 'man' is to 'nephew'\n",
      "'woman' is to 'chairwoman' as 'man' is to 'chairman'\n",
      "'woman' is to 'Mary' as 'man' is to 'Paul'\n",
      "'woman' is to 'sister' as 'man' is to 'brother'\n",
      "'woman' is to 'actress' as 'man' is to 'actor'\n",
      "Potentially Biased Analogies\n",
      "'woman' is to 'nurse' as 'man' is to 'medic'\n",
      "'woman' is to 'dancer' as 'man' is to 'magician'\n",
      "'woman' is to 'feminist' as 'man' is to 'anarchist'\n",
      "'woman' is to 'baking' as 'man' is to 'roasting'\n",
      "'woman' is to 'volleyball' as 'man' is to 'football'\n",
      "'woman' is to 'softball' as 'man' is to 'baseball'\n",
      "'woman' is to 'salon' as 'man' is to 'barber_shop'\n",
      "'woman' is to 'blond' as 'man' is to 'burly'\n",
      "'woman' is to 'cute' as 'man' is to 'goofy'\n",
      "'woman' is to 'beautiful' as 'man' is to 'magnificent'\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True]\n",
      "==================================================\n",
      "Debiased  Embeddings\n",
      "Appropriate Analogies\n",
      "'woman' is to 'her' as 'man' is to 'his'\n",
      "'woman' is to 'herself' as 'man' is to 'his'\n",
      "'woman' is to 'spokeswoman' as 'man' is to 'spokesman'\n",
      "'woman' is to 'daughter' as 'man' is to 'son'\n",
      "'woman' is to 'mother' as 'man' is to 'father'\n",
      "'woman' is to 'niece' as 'man' is to 'nephew'\n",
      "'woman' is to 'chairwoman' as 'man' is to 'chairman'\n",
      "'woman' is to 'Mary' as 'man' is to 'Father'\n",
      "'woman' is to 'sister' as 'man' is to 'brother'\n",
      "'woman' is to 'actress' as 'man' is to 'actor'\n",
      "Potentially Biased Analogies\n",
      "'woman' is to 'nurse' as 'man' is to 'gentleman'\n",
      "'woman' is to 'dancer' as 'man' is to 'dancers'\n",
      "'woman' is to 'feminist' as 'man' is to 'feminists'\n",
      "'woman' is to 'baking' as 'man' is to 'cooking'\n",
      "'woman' is to 'volleyball' as 'man' is to 'softball'\n",
      "'woman' is to 'softball' as 'man' is to 'Softball'\n",
      "'woman' is to 'salon' as 'man' is to 'hair_salon'\n",
      "'woman' is to 'blond' as 'man' is to 'blonde'\n",
      "'woman' is to 'cute' as 'man' is to 'fella'\n",
      "'woman' is to 'beautiful' as 'man' is to 'gorgeous'\n"
     ]
    }
   ],
   "source": [
    "class WordEmbeddingDebiaser:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_file_path,\n",
    "        definitional_file_path='./data/definitional_pairs.json',\n",
    "        equalize_file_path='./data/equalize_pairs.json',\n",
    "        gender_specific_file_path='./data/gender_specific_full.json'\n",
    "    ):\n",
    "\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            embedding_file_path, binary=True\n",
    "        )\n",
    "\n",
    "        # collect first 300000 words\n",
    "        self.words = sorted([w for w in self.model.vocab],\n",
    "                            key=lambda w: self.model.vocab[w].index)[:300000]\n",
    "\n",
    "        # all vectors in an array (same order as self.words)\n",
    "        self.vecs = np.array([self.model[w] for w in self.words])\n",
    "        tqdm.write('vectors loaded')\n",
    "        # should take 2-5 min depending on your machine\n",
    "\n",
    "        self.n, self.d = self.vecs.shape\n",
    "\n",
    "        # word to index dictionary\n",
    "        self.w2i = {w: i for i, w in enumerate(self.words)}\n",
    "\n",
    "        # Some relevant words sets required for debiasing\n",
    "        with open(definitional_file_path, \"r\") as f:\n",
    "            self.definition_pairs = json.load(f)\n",
    "\n",
    "        with open(equalize_file_path, \"r\") as f:\n",
    "            self.equalize_pairs = json.load(f)\n",
    "\n",
    "        with open(gender_specific_file_path, \"r\") as f:\n",
    "            self.gender_specific_words = json.load(f)\n",
    "        self._normalize()\n",
    "\n",
    "    # Some potentially helpful functions, you don't have to use/implement them.\n",
    "    def _normalize(self):\n",
    "        \"\"\"\n",
    "        normalize self.vecs\n",
    "        \"\"\"\n",
    "        self.vecs /= np.linalg.norm(self.vecs, axis=1)[:, np.newaxis]\n",
    "\n",
    "    def _drop(self, u, v):\n",
    "        \"\"\"\n",
    "        remove a direction v from u\n",
    "        \"\"\"\n",
    "        return u - v * u.dot(v) / v.dot(v)\n",
    "\n",
    "    def w2v(self, word):\n",
    "        \"\"\"\n",
    "        for a word, return its corresponding vector\n",
    "        \"\"\"\n",
    "        return self.vecs[self.w2i[word]]\n",
    "\n",
    "    def debias(self):\n",
    "        self.gender_direction = self.identify_gender_subspace()\n",
    "        self.neutralize()\n",
    "        self.equalize()\n",
    "\n",
    "    def identify_gender_subspace(self):\n",
    "        \"\"\"Using self.definitional_pairs to identify a gender axis (1 dimensional).\n",
    "\n",
    "          Output: a gender direction using definitonal pairs\n",
    "\n",
    "        ****Note****\n",
    "\n",
    "         no other unimported packages listed above are allowed, please use\n",
    "         numpy.linalg.svd for PCA\n",
    "\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        #get indexes of 'female' and 'male' related words in definitional pairs\n",
    "        index_list = [[self.words.index(f), self.words.index(m)] \\\n",
    "                      for f,m in self.definition_pairs] \n",
    "        \n",
    "        center_list = [[(self.vecs[pair[0]] + self.vecs[pair[1]])/2] \\\n",
    "                       for pair in index_list] #get average the data pair\n",
    "        \n",
    "        differences = [[self.vecs[index_list[i][0]] - center_list[i], \\\n",
    "                        self.vecs[index_list[i][1]] - center_list[i]] \\\n",
    "                       for i in range(len(center_list))] #calc dist away from mean\n",
    "        \n",
    "        differences = np.array([pair[i][0] for pair in differences for i in range(len(pair))]) #turn into array\n",
    "     \n",
    "        _, _, Vh = np.linalg.svd(differences, full_matrices=False) #SVD to compute PCA on the data\n",
    "        self.gend_direct = -Vh[0] #gender direction denoted by 1st eigenvector\n",
    "        '''Checked my self.pca against the code below for sklearn.decomposition.PCA method\n",
    "        pca = PCA(n_components=1)\n",
    "        pca = pca.fit(differences)\n",
    "        pca_vals = pca.components_[0]\n",
    "        print(self.gend_direct.round(2) == pca_vals.round(2)) #True\n",
    "        '''\n",
    "        # END OF SOLUTION\n",
    "\n",
    "    def neutralize(self):\n",
    "        \"\"\"Performing the neutralizing step: projecting all gender neurtal words away\n",
    "        from the gender direction\n",
    "\n",
    "        No output, please adjust self.vecs\n",
    "\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        #modify self.vecs only if the word is not in the gender specific words\n",
    "        self.vecs = np.array([self._drop(self.vecs[i], self.gend_direct) \\\n",
    "                              if self.words[i] not in self.gender_specific_words \\\n",
    "                              else self.vecs[i] for i in range(len(self.words))])\n",
    "        self._normalize()\n",
    "        # END OF SOLUTION\n",
    "\n",
    "    def equalize(self):\n",
    "        \"\"\"Performing the equalizing step: make sure all equalized pairs are\n",
    "        equaldistant to the gender direction.\n",
    "\n",
    "        No output, please adapt self.vecs\n",
    "\n",
    "        \"\"\"\n",
    "        # SOLUTION\n",
    "        dist = [(self.w2v(f) + self.w2v(m)) / 2 for f,m in self.equalize_pairs] #dist for words in equalize pairs\n",
    "        remove = [self._drop(dist[i], self.gend_direct) for i in range(len(dist))] #removing directions\n",
    "        \n",
    "        #distance metric to know how far to move the words to make them equidistant in step below\n",
    "        v = [np.sqrt(1 - np.linalg.norm(remove[i]) ** 2) if np.dot(dist[i]*2, self.gend_direct) < 0\\\n",
    "             else -np.sqrt(1 - np.linalg.norm(remove[i]) ** 2) for i in range(len(remove))]\n",
    "       \n",
    "        idx = 0\n",
    "        '''update self.vecs accordingly equalizing distance between words in \n",
    "           equalize pairs and gender neutral words for the gender subspace'''  \n",
    "        for f, m in self.equalize_pairs:\n",
    "            self.vecs[self.words.index(f)] = v[idx] * self.gend_direct + remove[idx]\n",
    "            self.vecs[self.words.index(m)] = -v[idx] * self.gend_direct + remove[idx]\n",
    "            idx += 1\n",
    "        self._normalize()\n",
    "        # END OF SOLUTION\n",
    "\n",
    "    def compute_analogy(self, w3, w1='woman', w2='man'):\n",
    "        \"\"\"input: w3, w1, w2, satifying the analogy w1: w2 :: w3 : w4\n",
    "\n",
    "        output: w4(a word string) which is the solution to the analogy (w4 is\n",
    "          constrained to be different from w1, w2 and w3)\n",
    "\n",
    "        \"\"\"\n",
    "        diff = self.w2v(w2) - self.w2v(w1)\n",
    "        vec = diff / np.linalg.norm(diff) + self.w2v(w3)\n",
    "        vec = vec / np.linalg.norm(vec)\n",
    "        if w3 == self.words[np.argsort(vec.dot(self.vecs.T))[-1]]:\n",
    "            return self.words[np.argsort(vec.dot(self.vecs.T))[-2]]\n",
    "        return self.words[np.argmax(vec.dot(self.vecs.T))]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Original Embedding\n",
    "\n",
    "    we = WordEmbeddingDebiaser('./data/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "    print('=' * 50)\n",
    "    print('Original Embeddings')\n",
    "    # she-he analogy evaluation\n",
    "    w3s1 = [\n",
    "        'her', 'herself', 'spokeswoman', 'daughter', 'mother', 'niece',\n",
    "        'chairwoman', 'Mary', 'sister', 'actress'\n",
    "    ]\n",
    "    w3s2 = [\n",
    "        'nurse', 'dancer', 'feminist', 'baking', 'volleyball', 'softball',\n",
    "        'salon', 'blond', 'cute', 'beautiful'\n",
    "    ]\n",
    "\n",
    "    w4s1 = [we.compute_analogy(w3) for w3 in w3s1]\n",
    "    w4s2 = [we.compute_analogy(w3) for w3 in w3s2]\n",
    "\n",
    "    print('Appropriate Analogies')\n",
    "    for w3, w4 in zip(w3s1, w4s1):\n",
    "        print(\"'woman' is to '%s' as 'man' is to '%s'\" % (w3, w4))\n",
    "\n",
    "    print('Potentially Biased Analogies')\n",
    "    for w3, w4 in zip(w3s2, w4s2):\n",
    "        print(\"'woman' is to '%s' as 'man' is to '%s'\" % (w3, w4))\n",
    "\n",
    "    we.debias()\n",
    "\n",
    "    print('=' * 50)\n",
    "    print('Debiased  Embeddings')\n",
    "    # she-he analogy evaluation\n",
    "    w4s1 = [we.compute_analogy(w3) for w3 in w3s1]\n",
    "    w4s2 = [we.compute_analogy(w3) for w3 in w3s2]\n",
    "\n",
    "    print('Appropriate Analogies')\n",
    "    for w3, w4 in zip(w3s1, w4s1):\n",
    "        print(\"'woman' is to '%s' as 'man' is to '%s'\" % (w3, w4))\n",
    "\n",
    "    print('Potentially Biased Analogies')\n",
    "    for w3, w4 in zip(w3s2, w4s2):\n",
    "        print(\"'woman' is to '%s' as 'man' is to '%s'\" % (w3, w4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMg1N8omErBF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SFDL_HW5_code_my_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
